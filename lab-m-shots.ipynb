{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b19fff-8f42-4e9f-a73e-00cff106805a",
   "metadata": {},
   "source": [
    "# M-Shots Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34723a72-1601-4685-a0ba-bff544425d48",
   "metadata": {
    "id": "34723a72-1601-4685-a0ba-bff544425d48"
   },
   "source": [
    "In this notebook, we'll explore small prompt engineering techniques and recommendations that will help us elicit responses from the models that are better suited to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fba193cc-d8a0-4ad2-8177-380204426859",
   "metadata": {
    "id": "fba193cc-d8a0-4ad2-8177-380204426859"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502cfc93-21e0-498f-9650-37bc6ddd514d",
   "metadata": {
    "id": "502cfc93-21e0-498f-9650-37bc6ddd514d"
   },
   "source": [
    "# Formatting the answer with Few Shot Samples.\n",
    "\n",
    "To obtain the model's response in a specific format, we have various options, but one of the most convenient is to use Few-Shot Samples. This involves presenting the model with pairs of user queries and example responses.\n",
    "\n",
    "Large models like GPT-3.5 respond well to the examples provided, adapting their response to the specified format.\n",
    "\n",
    "Depending on the number of examples given, this technique can be referred to as:\n",
    "* Zero-Shot.\n",
    "* One-Shot.\n",
    "* Few-Shots.\n",
    "\n",
    "With One Shot should be enough, and it is recommended to use a maximum of six shots. It's important to remember that this information is passed in each query and occupies space in the input prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a8344712-06d7-4c24-83d8-f36d62926e5e",
   "metadata": {
    "id": "a8344712-06d7-4c24-83d8-f36d62926e5e"
   },
   "outputs": [],
   "source": [
    "# Function to call the model.\n",
    "def return_OAIResponse(user_message, context):\n",
    "    client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "    newcontext = context.copy()\n",
    "    newcontext.append({'role':'user', 'content':\"question: \" + user_message})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=newcontext,\n",
    "            temperature=1,\n",
    "        )\n",
    "\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611d73d-9330-466d-b705-543667e1b561",
   "metadata": {
    "id": "f611d73d-9330-466d-b705-543667e1b561"
   },
   "source": [
    "In this zero-shots prompt we obtain a correct response, but without formatting, as the model incorporates the information he wants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
    "outputId": "4c4a9f4f-67c9-4a11-837f-1a1fd6b516ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Vettel won the F1 World Championship in 2010 driving for Red Bull Racing.\n"
     ]
    }
   ],
   "source": [
    "#zero-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert in F1.'}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2010?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8",
   "metadata": {
    "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8"
   },
   "source": [
    "For a model as large and good as GPT 3.5, a single shot is enough to learn the output format we expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
    "outputId": "5278df23-8797-4dc2-9340-ac29c1318a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Sebastian Vettel.\n",
      "Team: Red Bull Racing.\n"
     ]
    }
   ],
   "source": [
    "#one-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2000 f1 championship?\n",
    "     Driver: Michael Schumacher.\n",
    "     Team: Ferrari.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2011?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c454a8-181b-482b-873a-81d6ffde4674",
   "metadata": {
    "id": "32c454a8-181b-482b-873a-81d6ffde4674"
   },
   "source": [
    "Smaller models, or more complicated formats, may require more than one shot. Here a sample with two shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
    "outputId": "a6f90f5d-6d68-4b3d-ccb5-5848ae4e3e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Fernando Alonso.\n",
      "Team: Renault.\n"
     ]
    }
   ],
   "source": [
    "#Few shots\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
    "outputId": "75f63fe3-0efc-45ed-dd45-71dbbb08d7a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2019 F1 championship was won by Lewis Hamilton driving for Mercedes.\n"
     ]
    }
   ],
   "source": [
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86",
   "metadata": {
    "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86"
   },
   "source": [
    "We've been creating the prompt without using OpenAI's roles, and as we've seen, it worked correctly.\n",
    "\n",
    "However, the proper way to do this is by using these roles to construct the prompt, making the model's learning process even more effective.\n",
    "\n",
    "By not feeding it the entire prompt as if they were system commands, we enable the model to learn from a conversation, which is more realistic for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
    "outputId": "868d2040-ca3c-4a47-a1e8-1e08d581191d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton. \n",
      "Team: Mercedes. \n",
      "Points: 413.\n"
     ]
    }
   ],
   "source": [
    "#Recomended solution\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert in f1.\\n\\n'},\n",
    "    {'role':'user', 'content':'Who won the 2010 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Sebastian Vettel. \\nTeam: Red Bull. \\nPoints: 256. \"\"\"},\n",
    "    {'role':'user', 'content':'Who won the 2009 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Jenson Button. \\nTeam: BrawnGP. \\nPoints: 95. \"\"\"},\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a020cb",
   "metadata": {},
   "source": [
    "Extra polish (optional)\n",
    "Use temperature=0.2 in return_OAIResponse for steadier answers.\n",
    "Keep team naming consistent (e.g., “Red Bull” vs “Red Bull Renault”); consistency improves pattern following.\n",
    "If you need machine-readable results later, ask for JSON in the system message instead of plain text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a44e8",
   "metadata": {},
   "source": [
    "We could also address it by using a more conventional prompt, describing what we want and how we want the format. However, it's essential to understand that in this case, the model is following instructions, whereas in the case of use shots, it is learning in real-time during inference.;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "56230b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton   Team: Mercedes   Points: 413\n"
     ]
    }
   ],
   "source": [
    "context_user = [ \n",
    "    {'role':'system', 'content':\n",
    "        \"\"\"\n",
    "        You are and expert in f1. \n",
    "        You are going to answer the question of the user giving the name of the rider, \n",
    "        the name of the team and the points of the champion, following the format: Drive: Team: Points: \"\"\" } ] \n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f6b42-f351-496b-a7e8-1286426457eb",
   "metadata": {
    "id": "ac6f6b42-f351-496b-a7e8-1286426457eb"
   },
   "source": [
    "We could also address it by using a more conventional prompt, describing what we want and how we want the format.\n",
    "\n",
    "However, it's essential to understand that in this case, the model is following instructions, whereas in the case of use shots, it is learning in real-time during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
    "outputId": "4c970dde-37ff-41a9-8d4e-37bb727f47a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton\n",
      "Team: Mercedes\n",
      "Points: 413\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an expert in F1.\\n\"\n",
    "            \"Answer the user's question by giving the champion's details in EXACTLY three lines:\\n\"\n",
    "            \"Driver: <name>\\n\"\n",
    "            \"Team: <team>\\n\"\n",
    "            \"Points: <integer>\\n\"\n",
    "            \"Do not add extra text.\"\n",
    "        ),\n",
    "    }\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "KNDL1GzVngyL",
   "metadata": {
    "id": "KNDL1GzVngyL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Fernando Alonso.\n",
      "Team: Renault.\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are classifying .\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qZPNTLMPnkQ4",
   "metadata": {
    "id": "qZPNTLMPnkQ4"
   },
   "source": [
    "Few Shots for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ejcstgTxnnX5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejcstgTxnnX5",
    "outputId": "4b91cc73-18f6-4944-a46b-806b02b7becb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in reviewing product opinions and classifying them as positive or negative.\n",
    "\n",
    "     It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
    "     Sentiment: Positive\n",
    "\n",
    "     It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
    "     Sentiment: Negative.\n",
    "\n",
    "     I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
    "     Sentiment: Neutral\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"I'm not going to return it, but I don't plan to buy it again.\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15",
   "metadata": {
    "id": "ZHr_75sDqDJp"
   },
   "source": [
    "# Exercise\n",
    " - Complete the prompts similar to what we did in class. \n",
    "     - Try at least 3 versions\n",
    "     - Be creative\n",
    " - Write a one page report summarizing your findings.\n",
    "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
    " - What did you learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52787fba",
   "metadata": {},
   "source": [
    "# Version 1 — Few-shot like example (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9adda59c-ad09-4e9d-88cd-54f42384a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system','content':\n",
    "    \"\"\"You are an expert in reviewing product opinions and classifying them as positive, negative, or neutral.\n",
    "\n",
    "    It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
    "    Sentiment: Positive\n",
    "\n",
    "    It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
    "    Sentiment: Negative\n",
    "\n",
    "    I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
    "    Sentiment: Neutral\n",
    "    \"\"\"}\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\n",
    "    \"I'm not going to return it, but I don't plan to buy it again.\",\n",
    "    context_user\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98647434",
   "metadata": {},
   "source": [
    "# Version 2 — Structured system instruction + one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9fd642a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system','content':\n",
    "    \"\"\"You classify product reviews into {Positive, Negative, Neutral}.\n",
    "    Respond only with the sentiment label.\n",
    "\n",
    "    Example:\n",
    "    Review: \"It works great, I will buy again.\"\n",
    "    Answer: Positive\n",
    "    \"\"\"}\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\n",
    "    \"I'm not going to return it, but I don't plan to buy it again.\",\n",
    "    context_user\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19946156",
   "metadata": {},
   "source": [
    "# Version 3 — Output as JSON + explicit definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "613230c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"sentiment\": \"Neutral\"}\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system','content':\n",
    "    \"\"\"You are a sentiment classifier.\n",
    "    \n",
    "    Rules:\n",
    "    - Positive: reviewer intends to buy again or expresses satisfaction\n",
    "    - Negative: reviewer would not buy again or expresses dissatisfaction\n",
    "    - Neutral: unclear or mixed without a buying decision\n",
    "\n",
    "    Respond in JSON format:\n",
    "    {\"sentiment\": \"Positive|Negative|Neutral\"}\n",
    "    \"\"\"}\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\n",
    "    \"I'm not going to return it, but I don't plan to buy it again.\",\n",
    "    context_user\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal\n",
    "\n",
    "Test different prompting strategies to classify product review sentiment (positive/negative/neutral) and evaluate consistency, quality, and hallucination.\n",
    "\n",
    "Approach\n",
    "Version 1:\n",
    "Style: Few-shot examples\n",
    "Notes: Teaches through examples, similar to training\n",
    "\n",
    "Version 2:\n",
    "Style: Introduction + 1 example\n",
    "Notes: More compact - focus on task definition\n",
    "\n",
    "Version 3:\n",
    "Style: Structured JSON output\n",
    "Notes: Most precise + maschine readable\n",
    "\n",
    "\n",
    "Results\n",
    "The last two versions correctly classified the sentiment as Neutral.\n",
    "\n",
    "Learned:\n",
    "1. Prompt design changes the model's behavior.\n",
    "Even though the input sentence stays the same, the way you instruct the model dramatically affects the output.\n",
    "Few-shot prompting (examples) → model imitates the pattern → more accurate\n",
    "Pure instruction prompting → model reasons on its own → may misinterpret subtle cases\n",
    "Lesson: The model isn't “learning facts” — it is responding based on framing.\n",
    "\n",
    "2. Temperature matters for classification\n",
    "The used value for temperature = 1, which introduces randomness.\n",
    "At higher temperature, GPT is more “creative,” which is bad for precise tasks like sentiment classification.\n",
    "Low temperature = consistency\n",
    "High temperature = creative drift / misclassification\n",
    "\n",
    "3. Ambiguous input → model uncertainty\n",
    "The test phrase: \"I'm not going to return it, but I don't plan to buy it again.\"\n",
    "Expresses: Not terrible (not returning). However, not good enough to repurchase\n",
    "This is borderline between Neutral and Negative, so when the instructions changed, the model interpreted it differently.\n",
    "Lesson: Clear definitions improve consistency:\n",
    "“Would not buy again” = \n",
    "\n",
    "4. Few-shot examples guide the model better\n",
    "The version with sentiment examples got the correct output because the model copied the pattern.\n",
    "The later prompts lacked concrete examples, and the model relied on its own interpretation — producing Neutral.\n",
    "Lesson: Few-shot examples reduce ambiguity.\n",
    "\n",
    "Conclusion: \n",
    "\n",
    "The experiment showed:\n",
    "\n",
    "-  style strongly affects model output;\n",
    "- Few-shot learning leads to more consistent classification than plain instructions for subtle sentiment tasks\n",
    "- High temperature causes variability in results; classification tasks should use low temperature (0–0.3)\n",
    "- The model doesn't truly “learn” — it reacts to prompt framing in real-time\n",
    "- Final takeaway\n",
    "\n",
    "For sentiment analysis:\n",
    "It is best to use few-shot examples + low temperature for consistent and accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bbd0c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
